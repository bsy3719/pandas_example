{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'],\n",
    "                              value_serializer=lambda m: json.dumps(m).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\n",
    "    # ('eventType', b'\"loadDataEvent\"'),\n",
    "    ('contentType', b'\"application/json\"'),\n",
    "    ('eventType', b'\"loadDataEvent\"'),\n",
    "    ('spring_json_header_types', b'{\"eventType\": \"java.lang.String\", \"contentType\": \"java.lang.String\"}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_write_file_topic = \"dataWriteRequest\"\n",
    "response_write_file_topic = \"dataWriteResponse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/baek/git/datacentro/data-centro-python/sample/titanic_set.csv\"\n",
    "use_column =['PassengerId', 'Survived', 'Pclass', 'Name']\n",
    "column_type = ['int64', 'int64', 'int64', 'object']\n",
    "order_column = ['PassengerId', 'Survived', 'Pclass', 'Name']\n",
    "rename_column = ['PassengerId', 'Survived', 'Pclass', 'Name']\n",
    "write_path = \"/Users/baek/git/datacentro/data-centro-python/sample/write.csv\"\n",
    "sep = \",\"\n",
    "encoding = \"utf-8\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7ff57bdb7100>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dict(\n",
    "    path = path,\n",
    "    useColumn = use_column,\n",
    "    columnType = column_type,\n",
    "    orderColumn = order_column,\n",
    "    renameColumn = rename_column,\n",
    "    writePath = write_path,\n",
    "    sep = sep,\n",
    "    encoding = encoding,\n",
    ")\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'],\n",
    "                              value_serializer=lambda m: json.dumps(m).encode('utf-8'))\n",
    "\n",
    "producer.send(request_write_file_topic, sample, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = signature(producer.send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(topic, value=None, key=None, headers=None, partition=None, timestamp_ms=None)\n"
     ]
    }
   ],
   "source": [
    "print(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic\n",
      "value=None\n",
      "key=None\n",
      "headers=None\n",
      "partition=None\n",
      "timestamp_ms=None\n"
     ]
    }
   ],
   "source": [
    "for param in sig.parameters.values():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2d0411a2e62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "producer.send.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<code object send at 0x7fc5c7dd7030, file \"/Users/baek/opt/anaconda3/envs/datacentro_sample/lib/python3.8/site-packages/kafka/producer/kafka.py\", line 538>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.send.__code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Publish a message to a topic.\\n\\n        Arguments:\\n            topic (str): topic where the message will be published\\n            value (optional): message value. Must be type bytes, or be\\n                serializable to bytes via configured value_serializer. If value\\n                is None, key is required and message acts as a 'delete'.\\n                See kafka compaction documentation for more details:\\n                https://kafka.apache.org/documentation.html#compaction\\n                (compaction requires kafka >= 0.8.1)\\n            partition (int, optional): optionally specify a partition. If not\\n                set, the partition will be selected using the configured\\n                'partitioner'.\\n            key (optional): a key to associate with the message. Can be used to\\n                determine which partition to send the message to. If partition\\n                is None (and producer's partitioner config is left as default),\\n                then messages with the same key will be delivered to the same\\n                partition (but if key is None, partition is chosen randomly).\\n                Must be type bytes, or be serializable to bytes via configured\\n                key_serializer.\\n            headers (optional): a list of header key value pairs. List items\\n                are tuples of str key and bytes value.\\n            timestamp_ms (int, optional): epoch milliseconds (from Jan 1 1970 UTC)\\n                to use as the message timestamp. Defaults to current time.\\n\\n        Returns:\\n            FutureRecordMetadata: resolves to RecordMetadata\\n\\n        Raises:\\n            KafkaTimeoutError: if unable to fetch topic metadata, or unable\\n                to obtain memory buffer prior to configured max_block_ms\\n        \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.send.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'send'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.send.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KafkaProducer.send'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.send.__qualname__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<function KafkaProducer.send at 0x7fc5c7df3b80>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(producer.send.__func__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kafka.producer.kafka'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.send.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'send(topic, value=None, key=None, headers=None, partition=None, timestamp_ms=None)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.send.__name__ + str(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    def send(self, topic, value=None, key=None, headers=None, partition=None, timestamp_ms=None):\\n        \"\"\"Publish a message to a topic.\\n\\n        Arguments:\\n            topic (str): topic where the message will be published\\n            value (optional): message value. Must be type bytes, or be\\n                serializable to bytes via configured value_serializer. If value\\n                is None, key is required and message acts as a \\'delete\\'.\\n                See kafka compaction documentation for more details:\\n                https://kafka.apache.org/documentation.html#compaction\\n                (compaction requires kafka >= 0.8.1)\\n            partition (int, optional): optionally specify a partition. If not\\n                set, the partition will be selected using the configured\\n                \\'partitioner\\'.\\n            key (optional): a key to associate with the message. Can be used to\\n                determine which partition to send the message to. If partition\\n                is None (and producer\\'s partitioner config is left as default),\\n                then messages with the same key will be delivered to the same\\n                partition (but if key is None, partition is chosen randomly).\\n                Must be type bytes, or be serializable to bytes via configured\\n                key_serializer.\\n            headers (optional): a list of header key value pairs. List items\\n                are tuples of str key and bytes value.\\n            timestamp_ms (int, optional): epoch milliseconds (from Jan 1 1970 UTC)\\n                to use as the message timestamp. Defaults to current time.\\n\\n        Returns:\\n            FutureRecordMetadata: resolves to RecordMetadata\\n\\n        Raises:\\n            KafkaTimeoutError: if unable to fetch topic metadata, or unable\\n                to obtain memory buffer prior to configured max_block_ms\\n        \"\"\"\\n        assert value is not None or self.config[\\'api_version\\'] >= (0, 8, 1), (\\n            \\'Null messages require kafka >= 0.8.1\\')\\n        assert not (value is None and key is None), \\'Need at least one: key or value\\'\\n        key_bytes = value_bytes = None\\n        try:\\n            self._wait_on_metadata(topic, self.config[\\'max_block_ms\\'] / 1000.0)\\n\\n            key_bytes = self._serialize(\\n                self.config[\\'key_serializer\\'],\\n                topic, key)\\n            value_bytes = self._serialize(\\n                self.config[\\'value_serializer\\'],\\n                topic, value)\\n            assert type(key_bytes) in (bytes, bytearray, memoryview, type(None))\\n            assert type(value_bytes) in (bytes, bytearray, memoryview, type(None))\\n\\n            partition = self._partition(topic, partition, key, value,\\n                                        key_bytes, value_bytes)\\n\\n            if headers is None:\\n                headers = []\\n            assert type(headers) == list\\n            assert all(type(item) == tuple and len(item) == 2 and type(item[0]) == str and type(item[1]) == bytes for item in headers)\\n\\n            message_size = self._estimate_size_in_bytes(key_bytes, value_bytes, headers)\\n            self._ensure_valid_record_size(message_size)\\n\\n            tp = TopicPartition(topic, partition)\\n            log.debug(\"Sending (key=%r value=%r headers=%r) to %s\", key, value, headers, tp)\\n            result = self._accumulator.append(tp, timestamp_ms,\\n                                              key_bytes, value_bytes, headers,\\n                                              self.config[\\'max_block_ms\\'],\\n                                              estimated_size=message_size)\\n            future, batch_is_full, new_batch_created = result\\n            if batch_is_full or new_batch_created:\\n                log.debug(\"Waking up the sender since %s is either full or\"\\n                          \" getting a new batch\", tp)\\n                self._sender.wakeup()\\n\\n            return future\\n            # handling exceptions and record the errors;\\n            # for API exceptions return them in the future,\\n            # for other exceptions raise directly\\n        except Errors.BrokerResponseError as e:\\n            log.debug(\"Exception occurred during message send: %s\", e)\\n            return FutureRecordMetadata(\\n                FutureProduceResult(TopicPartition(topic, partition)),\\n                -1, None, None,\\n                len(key_bytes) if key_bytes is not None else -1,\\n                len(value_bytes) if value_bytes is not None else -1,\\n                sum(len(h_key.encode(\"utf-8\")) + len(h_value) for h_key, h_value in headers) if headers else -1,\\n            ).failure(e)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "inspect.getsource(producer.send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"col1(columnName) = 첫번째 컬럼명, col2(columnName) = 두번째 컬럼명\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['col1(columnName) = 첫번째 컬럼명', ' col2(columnName) = 두번째 컬럼명']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.split(\",\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = list(map(lambda x : x.strip(), test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['col1(columnName) = 첫번째 컬럼명', 'col2(columnName) = 두번째 컬럼명']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacentro_sample",
   "language": "python",
   "name": "datacentro_sample"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
